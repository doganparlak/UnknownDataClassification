{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfHD8msuuyKj",
        "outputId": "693dafdd-3c1b-41e9-cdc2-3ed1cce62cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Attention, Concatenate, Reshape\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.regularizers import l1, l2, L1L2\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.initializers import GlorotUniform\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, log_loss, make_scorer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import load_model\n",
        "from scipy.optimize import minimize\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "from skopt import BayesSearchCV\n",
        "import joblib\n",
        "import json\n",
        "import zipfile\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vpkZ0eEuUHy"
      },
      "outputs": [],
      "source": [
        "#!pip install umap-learn\n",
        "#!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C30dVy1xw4ex"
      },
      "source": [
        "# Data Processs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwJQD2NMu3Nl"
      },
      "outputs": [],
      "source": [
        "def data_loader(path = \"/content/train_data_swc.csv\"):\n",
        "    df = pd.read_csv(path)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI9UXh3tu5KV"
      },
      "outputs": [],
      "source": [
        "def get_new_train_test(X_train, y_train):\n",
        "    # Shuffle and split the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POgAunWIs4yh"
      },
      "outputs": [],
      "source": [
        "def umap_feature_eng(train2, test2, test, n_components = 3, n_neighbors = 15, names = ['train2_data_umap3.csv','test2_data_umap3.csv' ,'test_data_umap3.csv']):\n",
        "  umap_model = umap.UMAP(n_components=n_components, n_neighbors = n_neighbors, n_jobs=-1)\n",
        "  train2_embeddings = umap_model.fit_transform(train2)\n",
        "  print(\"train2 fitted\")\n",
        "  test2_embeddings = umap_model.transform(test2)\n",
        "  print(\"test2 transformed\")\n",
        "  test_embeddings = umap_model.transform(test)\n",
        "  print(\"test transformed\")\n",
        "\n",
        "  new_cols = []\n",
        "  for i in range(n_components):\n",
        "      new_cols.append(f'umap_{i+1}')\n",
        "\n",
        "  # Convert embeddings to DataFrame\n",
        "  train2_embedding_df = pd.DataFrame(train2_embeddings, columns=new_cols)\n",
        "  test2_embedding_df = pd.DataFrame(test2_embeddings, columns=new_cols)\n",
        "  test_embedding_df = pd.DataFrame(test_embeddings, columns=new_cols)\n",
        "\n",
        "  # Concatenate the embeddings DataFrame with the original DataFrame\n",
        "  train2_with_embeddings = pd.concat([train2, train2_embedding_df], axis=1)\n",
        "  test2_with_embeddings = pd.concat([test2, test2_embedding_df], axis=1)\n",
        "  test_with_embeddings = pd.concat([test, test_embedding_df], axis=1)\n",
        "\n",
        "  # Save new data frame\n",
        "  train2_with_embeddings.to_csv('/content/drive/My Drive/swc_data/' + names[0], index=False)\n",
        "  test2_with_embeddings.to_csv('/content/drive/My Drive/swc_data/' + names[1], index=False)\n",
        "  test_with_embeddings.to_csv('/content/drive/My Drive/swc_data/' + names[2], index=False)\n",
        "\n",
        "  return train2_with_embeddings, test2_with_embeddings, test_with_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnKu7TRqBClj"
      },
      "outputs": [],
      "source": [
        "def normalize_data(X_train2, X_test2, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    X_train2_scaled = scaler.fit_transform(X_train2.values)\n",
        "    X_test2_scaled = scaler.transform(X_test2.values)\n",
        "    X_test_scaled = scaler.transform(X_test.values)\n",
        "    X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=features)\n",
        "    X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=features)\n",
        "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=features)\n",
        "\n",
        "    return X_train2_scaled, X_test2_scaled, X_test_scaled\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4BNVN7Iu8Rs"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/My Drive/swc_data/'\n",
        "train_data = data_loader(folder_path + \"train_data_swc.csv\")\n",
        "test_data = data_loader(folder_path + \"test_data_swc.csv\")\n",
        "y_train2  = data_loader(folder_path + \"y_train2.csv\")\n",
        "y_test2  = data_loader(folder_path + \"y_test2.csv\")\n",
        "\n",
        "X_train2_umap3 = data_loader(folder_path + \"train2_data_umap3.csv\")\n",
        "X_test2_umap3 = data_loader(folder_path + \"test2_data_umap3.csv\")\n",
        "X_test_umap3 = data_loader(folder_path + \"test_data_umap3.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DbYSOpSvHRa"
      },
      "outputs": [],
      "source": [
        "X_train2, X_test2, _, _ = get_new_train_test(train_data.drop(\"y\", axis = 1), train_data[\"y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5MAVD35vAPu"
      },
      "outputs": [],
      "source": [
        "X_train2, X_test2, X_test = normalize_data(X_train2, X_test2, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDivFLmiFwOU",
        "outputId": "7c2194c9-cc68-4198-c7fd-46f1a5bf7f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train2 fitted\n",
            "test2 transformed\n",
            "test transformed\n"
          ]
        }
      ],
      "source": [
        "X_train2_umap10, X_test2_umap10, X_test_umap10 = umap_feature_eng(X_train2, X_test2, X_test, n_components = 10, n_neighbors = 15, names = ['train2_data_umap10.csv','test2_data_umap10.csv' ,'test_data_umap10.csv'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNKRQHQWlWAy"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4aYMJV0lZvm"
      },
      "outputs": [],
      "source": [
        "def train_rf(X_train2, y_train2, X_test2, y_test2):\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    X_train2_scaled = scaler.fit_transform(X_train2.values)\n",
        "    X_test2_scaled = scaler.transform(X_test2.values)\n",
        "    X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=features)\n",
        "    X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=features)\n",
        "    print(\"Data Normalized\")\n",
        "\n",
        "    # Number of samples for each class\n",
        "    class_samples = y_train2[\"y\"].value_counts().to_dict()\n",
        "\n",
        "    # Calculate class weights\n",
        "    total_samples = sum(class_samples.values())\n",
        "    class_weights = {class_label: total_samples / (len(class_samples) * class_samples[class_label]) for class_label in class_samples}\n",
        "\n",
        "\n",
        "    # Parameter initializations\n",
        "    n_splits = 3\n",
        "\n",
        "    # Initialize k-fold cross-validation\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    # Hyperparameter search space for Bayesian optimization\n",
        "    param_space = {\n",
        "        'n_estimators': (100, 1500),\n",
        "        'max_depth': (2, 250),\n",
        "        'min_samples_split': (2, 100),\n",
        "        'min_samples_leaf': (1, 100),\n",
        "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
        "        'max_features': [None, \"sqrt\", \"log2\"]\n",
        "    }\n",
        "\n",
        "    # Initialize Random Forest classifier\n",
        "    clf = RandomForestClassifier(random_state=42, class_weight=class_weights, n_jobs=-1)\n",
        "\n",
        "    # Define log loss scorer\n",
        "    log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
        "\n",
        "    # Initialize Bayesian search\n",
        "    bayes_search = BayesSearchCV(clf, param_space, cv=kf, scoring=log_loss_scorer, n_iter= 35, verbose=1, n_jobs=-1)\n",
        "\n",
        "    # Fit BayesSearchCV to data\n",
        "    print(\"Bayesian Search Begin\")\n",
        "    bayes_search.fit(X_train2_scaled.values, y_train2[\"y\"].ravel())\n",
        "\n",
        "    # Print and save the best parameters found\n",
        "    print(\"Best parameters found:\")\n",
        "    best_params = bayes_search.best_params_\n",
        "    print(best_params)\n",
        "\n",
        "    # Retrieve the best estimator and save it\n",
        "    best_clf = bayes_search.best_estimator_\n",
        "    random_forest_model_path = '/content/drive/My Drive/swc_models/trained_model_random_forest_umap2joblib'\n",
        "    joblib.dump(best_clf, random_forest_model_path)\n",
        "\n",
        "    \"\"\"\n",
        "    best_clf = RandomForestClassifier(random_state=42, class_weight=class_weights, n_estimators = 1200, max_depth = 136,\n",
        "                                      min_samples_leaf = 1, min_samples_split = 2, n_jobs=-1) # 0.5473\n",
        "\n",
        "    best_clf.fit(X_train2_scaled, y_train2[\"y\"].ravel())\n",
        "    \"\"\"\n",
        "\n",
        "    # Validation Score\n",
        "    y_pred_val = best_clf.predict(X_test2_scaled)\n",
        "    y_prob_val = best_clf.predict_proba(X_test2_scaled)\n",
        "    # Calculate accuracy score\n",
        "    accuracy = accuracy_score(y_test2[\"y\"].ravel(), y_pred_val)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    # Calculate log-loss\n",
        "    logloss = log_loss(y_test2[\"y\"].ravel(), y_prob_val)\n",
        "    print(f\"Log Loss: {logloss:.4f}\")\n",
        "    return best_clf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZHn5FV-mI8L"
      },
      "outputs": [],
      "source": [
        "best_rf = train_rf(X_train2_umap3, y_train2, X_test2_umap3, y_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeryjjU1bdu9"
      },
      "source": [
        "## LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx1LKppgbhhE"
      },
      "outputs": [],
      "source": [
        "def train_lightgbm(X_train2, y_train2, X_test2, y_test2):\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    X_train2_scaled = scaler.fit_transform(X_train2.values)\n",
        "    X_test2_scaled = scaler.transform(X_test2.values)\n",
        "    X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=features)\n",
        "    X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=features)\n",
        "    print(\"Data Normalized\")\n",
        "\n",
        "\n",
        "    # Number of samples for each class\n",
        "    class_samples = y_train2[\"y\"].value_counts().to_dict()#\n",
        "\n",
        "    # Calculate class weights\n",
        "    total_samples = sum(class_samples.values())\n",
        "    class_weights = {class_label: total_samples / (len(class_samples) * class_samples[class_label]) for class_label in class_samples}\n",
        "\n",
        "\n",
        "    #Parameter initializations\n",
        "    n_splits = 3\n",
        "\n",
        "    # Initialize k-fold cross-validation\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    # Hyperparameter search space for Bayesian optimization\n",
        "    param_space = {\n",
        "        'num_leaves': (2, 1500),\n",
        "        'max_depth': (2, 100),\n",
        "        'learning_rate': (0.01, 0.3),\n",
        "        'min_child_samples': (5, 150),\n",
        "        'n_estimators': (100, 1000),\n",
        "        'subsample': (0.2, 1),\n",
        "        'colsample_bytree': (0.2, 1),\n",
        "    }\n",
        "\n",
        "     # Initialize LightGBM classifier\n",
        "    clf = lgb.LGBMClassifier(random_state=42, class_weight=class_weights, objective='multiclass', verbose = -1)\n",
        "\n",
        "    # Define log loss scorer\n",
        "    log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
        "\n",
        "    #Initialize bayesian search\n",
        "    bayes_search = BayesSearchCV(clf, param_space, cv=kf, scoring=log_loss_scorer, n_iter= 30, verbose= 1, n_jobs=-1)\n",
        "\n",
        "    # Fit BayesSearchCV to data\n",
        "    print(\"Bayesian Search Begin\")\n",
        "    bayes_search.fit(X_train2_scaled, y_train2[\"y\"].ravel())\n",
        "\n",
        "    # Print and save the best parameters found\n",
        "    print(\"Best parameters found:\")\n",
        "    best_params = bayes_search.best_params_\n",
        "    print(best_params)\n",
        "\n",
        "    # Retrieve the best estimator and save it\n",
        "    best_clf = bayes_search.best_estimator_\n",
        "    lightgbm_model_path = '/content/drive/My Drive/swc_models/trained_model_lightgbm_umap2.joblib'\n",
        "    joblib.dump(best_clf, lightgbm_model_path)\n",
        "\n",
        "    \"\"\"\n",
        "    best_clf = lgb.LGBMClassifier(num_leaves = 1000, max_depth = 45, min_child_samples = 87, n_estimators=192, learning_rate = 0.05,\n",
        "                                  random_state=42, objective='multiclass', verbose = -1, class_weight=class_weights,\n",
        "                                  colsample_bytree = 0.3202803930164253, subsample = 0.5934691217094938, n_jobs = -1) #0.4832\n",
        "\n",
        "\n",
        "    best_clf.fit(X_train2_scaled, y_train2[\"y\"])#\n",
        "    \"\"\"\n",
        "\n",
        "    # Validation Score\n",
        "    y_pred_val = best_clf.predict(X_test2_scaled)\n",
        "    y_prob_val = best_clf.predict_proba(X_test2_scaled)\n",
        "    # Calculate accuracy score for this fold\n",
        "    accuracy = accuracy_score(y_test2[\"y\"].ravel(), y_pred_val)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    # Calculate log-loss for this fold\n",
        "    logloss = log_loss(y_test2[\"y\"].ravel(), y_prob_val)\n",
        "    print(f\"Log Loss: {logloss:.4f}\")\n",
        "\n",
        "    return best_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayJirEB7bq55",
        "outputId": "d9c3771b-0d39-401b-b951-63985889a79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Normalized\n",
            "Bayesian Search Begin\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
          ]
        }
      ],
      "source": [
        "best_lightgbm = train_lightgbm(X_train2_umap10, y_train2, X_test2_umap10, y_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHnlykjZo1OY"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ho3PJi2o7nM"
      },
      "outputs": [],
      "source": [
        "def train_xgboost(X_train2, y_train2, X_test2, y_test2):\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    X_train2_scaled = scaler.fit_transform(X_train2.values)\n",
        "    X_test2_scaled = scaler.transform(X_test2.values)\n",
        "    X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=features)\n",
        "    X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=features)\n",
        "    print(\"Data Normalized\")\n",
        "\n",
        "\n",
        "    # Number of samples for each class\n",
        "    class_samples = (y_train2[\"y\"] - 1).value_counts().to_dict()\n",
        "\n",
        "    # Calculate class weights\n",
        "    total_samples = sum(class_samples.values())\n",
        "    class_weights = {class_label: total_samples / (len(class_samples) * class_samples[class_label]) for class_label in class_samples}\n",
        "\n",
        "\n",
        "    #Parameter initializations\n",
        "    n_splits = 3\n",
        "\n",
        "    # Initialize k-fold cross-validation\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "    # Hyperparameter search space for Bayesian optimization\n",
        "    param_space = {\n",
        "        'max_depth': (5, 50),\n",
        "        'learning_rate': (0.01, 0.5),\n",
        "        'min_child_weight': (2, 50),\n",
        "        'n_estimators': (25, 500),\n",
        "        'subsample': (0.2, 1),\n",
        "        'colsample_bytree': (0.2, 1)\n",
        "    }\n",
        "\n",
        "     # Initialize LightGBM classifier\n",
        "    clf = xgb.XGBClassifier(random_state=42, scale_pos_weight=class_weights, objective='multi:softprob', verbosity=0)\n",
        "\n",
        "    # Define log loss scorer\n",
        "    log_loss_scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n",
        "\n",
        "    #Initialize bayesian search\n",
        "    bayes_search = BayesSearchCV(clf, param_space, cv=kf, scoring=log_loss_scorer, n_iter= 50, verbose= 1, n_jobs=-1)\n",
        "\n",
        "    # Fit BayesSearchCV to data\n",
        "    print(\"Bayesian Search Begin\")\n",
        "    bayes_search.fit(X_train2_scaled, y_train2[\"y\"].ravel() - 1)\n",
        "\n",
        "    # Print and save the best parameters found\n",
        "    print(\"Best parameters found:\")\n",
        "    best_params = bayes_search.best_params_\n",
        "    print(best_params)\n",
        "\n",
        "    # Retrieve the best estimator and save it\n",
        "    best_clf = bayes_search.best_estimator_\n",
        "\n",
        "    xgboost_model_path = '/content/drive/My Drive/swc_models/trained_model_xgboost_tsne_umap.joblib'\n",
        "    joblib.dump(best_clf, xgboost_model_path)\n",
        "    \"\"\"\n",
        "    best_clf = xgb.XGBClassifier(random_state=42, scale_pos_weight=class_weights, objective='multi:softprob', verbosity=0,\n",
        "                                 max_depth = 25, learning_rate = 0.0923789893340372, min_child_weight = 25, n_estimators = 296,\n",
        "                                 subsample = 0.8742364404441483, colsample_bytree = 0.3445600498256228) #0.4741\n",
        "\n",
        "    best_clf.fit(X_train2_scaled, y_train2[\"y\"].ravel() - 1)\n",
        "    \"\"\"\n",
        "    # Validation Score\n",
        "    y_pred_val = best_clf.predict(X_test2_scaled)\n",
        "    y_prob_val = best_clf.predict_proba(X_test2_scaled)\n",
        "    # Calculate accuracy score for this fold\n",
        "    accuracy = accuracy_score(y_test2[\"y\"].ravel() - 1, y_pred_val)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    # Calculate log-loss for this fold\n",
        "    logloss = log_loss(y_test2[\"y\"].ravel() - 1, y_prob_val)\n",
        "    print(f\"Log Loss: {logloss:.4f}\")\n",
        "\n",
        "    return best_clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2lf9JvHrpip",
        "outputId": "a2d1fdd0-e269-4797-f366-354deea47754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Normalized\n",
            "Accuracy: 0.8226\n",
            "Log Loss: 0.4761\n"
          ]
        }
      ],
      "source": [
        "best_xgboost = train_xgboost(X_train2_umap3_50, y_train2, X_test2_umap3_50, y_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPJzHOTfw9ZV"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62jLNV6W37i9"
      },
      "outputs": [],
      "source": [
        "def train_neural_network(X_train2, y_train2, X_test2, y_test2):\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    X_train2_scaled = scaler.fit_transform(X_train2.values)\n",
        "    X_test2_scaled = scaler.transform(X_test2.values)\n",
        "    X_train2_scaled = pd.DataFrame(X_train2_scaled, columns=features)\n",
        "    X_test2_scaled = pd.DataFrame(X_test2_scaled, columns=features)\n",
        "    y_train2 = to_categorical(y_train2[\"y\"])[:,1:]\n",
        "    y_test2 = to_categorical(y_test2[\"y\"])[:,1:]\n",
        "    print(\"Data Normalized\")\n",
        "\n",
        "    model = Sequential([\n",
        "      Dense(1000, activation='relu', input_shape=(111,), kernel_regularizer=l1(0.0001)), # L1 regularization with coefficient 0.001\n",
        "      BatchNormalization(),\n",
        "      Dropout(0.3),\n",
        "      Dense(500, activation='relu', kernel_regularizer=l2(0.0001)), # L2 regularization with coefficient 0.001\n",
        "      BatchNormalization(),\n",
        "      Dropout(0.2),\n",
        "      Dense(250, activation='relu', kernel_regularizer=L1L2(l1=0.0001, l2=0.0001)), # L1 and L2 regularization with coefficients 0.001\n",
        "      BatchNormalization(),\n",
        "      Dropout(0.1),\n",
        "      Dense(125, activation='relu', kernel_regularizer=l2(0.0001)), # L2 regularization with coefficient 0.001\n",
        "      BatchNormalization(),\n",
        "      Dense(9, activation='softmax')  # Assuming 9 classes\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience= 40, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train2_scaled, y_train2, epochs=1250, batch_size=128, validation_split=0.2, callbacks=[early_stopping]) # batch_size 128\n",
        "\n",
        "    # Save the trained model using joblib\n",
        "    model.save('/content/drive/My Drive/swc_models/' + 'mlp_model_umap2.keras')\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "    # Evaluate the model\n",
        "    loss, accuracy = model.evaluate(X_test2_scaled, y_test2)\n",
        "    print(f'Test Loss: {loss:.3f}, Test Accuracy: {accuracy:.3f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I70IwxvX8klP"
      },
      "outputs": [],
      "source": [
        "train_neural_network(X_train2_umap3, y_train2, X_test2_umap3, y_test2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHfVf-C8oMgh"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7miPdPPIreG"
      },
      "outputs": [],
      "source": [
        "def load_models(folder_path = '/content/drive/My Drive/swc_models/'):\n",
        "    lightgbm_model = joblib.load(folder_path + 'trained_model_lightgbm_umap2.joblib')\n",
        "    mlp_model = load_model(folder_path + 'mlp_model_umap.keras')\n",
        "    random_forest_model = joblib.load(folder_path + 'trained_model_random_forest_umap.joblib')\n",
        "    xgboost_model = joblib.load(folder_path + 'trained_model_xgboost_umap2.joblib')\n",
        "    models = [lightgbm_model, mlp_model, random_forest_model, xgboost_model]\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKOmN74ZMC5s"
      },
      "outputs": [],
      "source": [
        "def validate(X_train2, X_test2, y_test2):\n",
        "    # Define the weights\n",
        "    models = load_models()\n",
        "    # Load the saved models\n",
        "    lightgbm_model = models[0]\n",
        "    mlp_model = models[1]\n",
        "    random_forest_model = models[2]\n",
        "    xgboost_model = models[3]\n",
        "    # Predictions on Validation Set\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    _ = scaler.fit_transform(X_train2.values)\n",
        "    X_test2 = scaler.transform(X_test2.values)\n",
        "    X_test2 = pd.DataFrame(X_test2, columns=features)\n",
        "    print(\"Data Normalized\")\n",
        "    light_gbm_prob_val = lightgbm_model.predict_proba(X_test2)\n",
        "    rf_prob_val = random_forest_model.predict_proba(X_test2)\n",
        "    mlp_prob_val = mlp_model.predict(X_test2)\n",
        "    xgboost_prob_val = xgboost_model.predict_proba(X_test2)\n",
        "\n",
        "    print(\"Performance of Base Models:\")\n",
        "    for name, prob_val in zip([\"Light GBM\", \"Random Forest\", \"MLP\", \"XGBoost\"],\n",
        "                              [light_gbm_prob_val, rf_prob_val, mlp_prob_val, xgboost_prob_val]):\n",
        "        accuracy = accuracy_score(y_test2, np.argmax(prob_val, axis=1) + 1)\n",
        "        logloss = log_loss(y_test2 - 1, prob_val)\n",
        "        print(f\"{name} - Accuracy: {accuracy:.4f}, Log Loss: {logloss:.4f}\")\n",
        "\n",
        "\n",
        "    return light_gbm_prob_val, rf_prob_val,  mlp_prob_val, xgboost_prob_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNpsKC7dHZEZ"
      },
      "outputs": [],
      "source": [
        "def optimal_weight_finder(light_gbm_prob_val, rf_prob_val, mlp_prob_val, xgboost_prob_val):\n",
        "  def compute_ensemble_logloss(weights):\n",
        "\n",
        "      weights = np.array(weights) / np.sum(weights)\n",
        "      # Compute ensemble probabilities\n",
        "      ensemble_proba_val = (weights[0] * light_gbm_prob_val +\n",
        "                            weights[1] * mlp_prob_val +\n",
        "                            weights[2] * rf_prob_val +\n",
        "                            weights[3] * xgboost_prob_val)\n",
        "\n",
        "      return log_loss(y_test2 - 1, ensemble_proba_val)\n",
        "\n",
        "\n",
        "  # Define the search space for weights\n",
        "  space = [Real(0, 1, name='lightgbm_weight'),\n",
        "           Real(0, 1, name='mlp_weight'),\n",
        "           Real(0, 1, name='random_forest_weight'),\n",
        "           Real(0, 1, name='xgboost_weight')]\n",
        "\n",
        "  # Perform Bayesian Optimization\n",
        "  result = gp_minimize(compute_ensemble_logloss, space, n_calls=175, random_state=42, verbose = True)\n",
        "\n",
        "  # Extract the optimal weights\n",
        "  optimal_weights = result.x\n",
        "  optimal_weights = optimal_weights / np.sum(optimal_weights)\n",
        "\n",
        "  print(\"Optimal Ensemble Log Loss:\", result.fun)\n",
        "  print(\"Optimal Weights:\", optimal_weights)\n",
        "\n",
        "  return optimal_weights\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HFM4JpfUlfd"
      },
      "outputs": [],
      "source": [
        "def ensemble_weight(X_train2, X_test2, X_test, y_test2, y_train2, optimal_weights, pred_no = 1):\n",
        "\n",
        "    # Load the saved models\n",
        "    models = load_models()\n",
        "    lightgbm_model = models[0]\n",
        "    mlp_model = models[1]\n",
        "    random_forest_model = models[2]\n",
        "    xgboost_model = models[3]\n",
        "\n",
        "    # Scale the data\n",
        "    scaler = StandardScaler()\n",
        "    features = X_train2.columns\n",
        "    _ = scaler.fit_transform(X_train2.values)\n",
        "    X_test = scaler.transform(X_test.values)\n",
        "    X_test2 = scaler.transform(X_test2.values)\n",
        "    X_test = pd.DataFrame(X_test, columns=features)\n",
        "    X_test2 = pd.DataFrame(X_test2, columns=features)\n",
        "    print(\"Data Normalized\")\n",
        "\n",
        "    # Make predictions using the ensemble on X_test\n",
        "    light_gbm_prob_test = np.load(\"/content/drive/My Drive/swc_predictions/lightgbm_test_prediction.npy\")\n",
        "    print(\"LightGBM predicted\")\n",
        "    rf_prob_test = random_forest_model.predict_proba(X_test)\n",
        "    print(\"Random Forest predicted\")\n",
        "    mlp_prob_test = mlp_model.predict(X_test)\n",
        "    print(\"MLP predicted\")\n",
        "    xgboost_prob_test = xgboost_model.predict_proba(X_test)\n",
        "    print(\"XGBoost predicted\")\n",
        "    #light_gbm_prob_test = lightgbm_model.predict_proba(X_test)\n",
        "\n",
        "\n",
        "    ensemble_proba_test = (optimal_weights[0] * light_gbm_prob_test +\n",
        "                           optimal_weights[1] * mlp_prob_test +\n",
        "                           optimal_weights[2] * rf_prob_test +\n",
        "                           optimal_weights[3] * xgboost_prob_test)\n",
        "\n",
        "    print(\"Final Prediction Made\")\n",
        "    column_names = \"c1,c2,c3,c4,c5,c6,c7,c8,c9\"\n",
        "    np.savetxt(f\"/content/drive/My Drive/swc_predictions/dogan_parlak_{pred_no}.csv\", ensemble_proba_test, delimiter=',', header=column_names, comments='')\n",
        "    print(\"Results Saved\")\n",
        "    return ensemble_proba_test, light_gbm_prob_test , rf_prob_test, mlp_prob_test, xgboost_prob_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tRedCMszvCK",
        "outputId": "fac76d55-a2da-42d6-9e95-19f3a8e9a21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Normalized\n",
            "391/391 [==============================] - 2s 4ms/step\n",
            "Performance of Base Models:\n",
            "Light GBM - Accuracy: 0.8208, Log Loss: 0.4700\n",
            "Random Forest - Accuracy: 0.8074, Log Loss: 0.5473\n",
            "MLP - Accuracy: 0.8079, Log Loss: 0.5085\n",
            "XGBoost - Accuracy: 0.8218, Log Loss: 0.4685\n"
          ]
        }
      ],
      "source": [
        "light_gbm_prob_val, rf_prob_val, mlp_prob_val, xgboost_prob_val = validate(X_train2_umap3, X_test2_umap3, y_test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtBW-35ldq4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146f57a3-971e-49c7-aef1-fe4183147d1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.3617517, 0.31526965, 0.02926238, 0.29371628]\n"
          ]
        }
      ],
      "source": [
        "#optimal_weights = optimal_weight_finder(light_gbm_prob_val, rf_prob_val, mlp_prob_val, xgboost_prob_val)\n",
        "optimal_weights= [0.3617517,  0.31526965, 0.02926238, 0.29371628]\n",
        "print(optimal_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2CnP5e0c0_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "030ad005-0f62-4583-c827-15835f4b57d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Normalized\n",
            "LightGBM predicted\n",
            "Random Forest predicted\n",
            "4688/4688 [==============================] - 22s 5ms/step\n",
            "MLP predicted\n",
            "XGBoost predicted\n",
            "Final Prediction Made\n",
            "Results Saved\n"
          ]
        }
      ],
      "source": [
        "ensemble_proba_test, light_gbm_prob_test, rf_prob_test, mlp_prob_test, xgboost_prob_test =\\\n",
        "ensemble_weight(X_train2_umap3, X_test2_umap3, X_test_umap3, y_test2, y_train2, optimal_weights, pred_no = 13)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_13 = pd.read_csv(f\"/content/drive/My Drive/swc_predictions/dogan_parlak_{13}.csv\")"
      ],
      "metadata": {
        "id": "4S7ElnYINo2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JmoHctN9qa"
      },
      "outputs": [],
      "source": [
        "pred_11 = pd.read_csv(f\"/content/drive/My Drive/swc_predictions/dogan_parlak_{11}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the predicted class for each row\n",
        "predicted_class_df11 = pred_11.idxmax(axis=1)\n",
        "predicted_class_df13 = pred_13.idxmax(axis=1)"
      ],
      "metadata": {
        "id": "cqdy3-VBdr99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter rows where the predicted class is the same in both DataFrames\n",
        "matching_rows = predicted_class_df11 == predicted_class_df13"
      ],
      "metadata": {
        "id": "JpFmVMrOe0PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "proba_diff = pred_13.lookup(pred_13.index[matching_rows], predicted_class_df13[matching_rows]) - \\\n",
        "             pred_11.lookup(pred_11.index[matching_rows], predicted_class_df11[matching_rows])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWts1suPe7af",
        "outputId": "910433b6-1d26-4b11-938d-20b23dcf6d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-1fb82d20bf08>:1: FutureWarning: The 'lookup' method is deprecated and will be removed in a future version. You can use DataFrame.melt and DataFrame.loc as a substitute.\n",
            "  proba_diff = pred_13.lookup(pred_13.index[matching_rows], predicted_class_df13[matching_rows]) - \\\n",
            "<ipython-input-55-1fb82d20bf08>:2: FutureWarning: The 'lookup' method is deprecated and will be removed in a future version. You can use DataFrame.melt and DataFrame.loc as a substitute.\n",
            "  pred_11.lookup(pred_11.index[matching_rows], predicted_class_df11[matching_rows])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(proba_diff > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzngakOfDWC",
        "outputId": "447874ac-c061-4e1c-c85d-47b567935bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70979"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LJzH56A7fO61"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qNKRQHQWlWAy",
        "zeryjjU1bdu9",
        "WPJzHOTfw9ZV",
        "rHfVf-C8oMgh"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}